#Στοιχεία : ΜΟΥΣΕΛΕ ΧΡΙΣΤΙΑΝΑ ΑΜ: 1090068 4ο έτος

#Κώδικας:

import pandas as pd
from google.colab import files
import nltk
import numpy as np
import matplotlib.pyplot as plt
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MaxAbsScaler
from sklearn.model_selection import KFold
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV

# Ανέβασε το αρχείο
uploaded = files.upload()

# Ορισμός ονόματος αρχείου
filename = "iphi2802 (3).csv"



def custom_error(y_true, y_pred):
    # Υπολογισμός απόκλισης από το κοντινότερο άκρο
    errors = np.zeros_like(y_true)
    for i in range(len(y_true)):
        min_error = abs(y_pred[i][0] - y_true.iloc[i]['date_min'])
        max_error = abs(y_pred[i][1] - y_true.iloc[i]['date_max'])
        errors[i] = min(min_error, max_error)
    return errors

# Διαβάζουμε τα δεδομένα από το αρχείο CSV, ορίζοντας τη μηχανή ως "python" 
data = pd.read_csv(filename , sep="	", engine="python", header=0, encoding="utf-8")



# Ορίζουμε μια λίστα για να αποθηκεύσουμε τα tokens των επιγραφών
tokens_list = []

for text in data['text']:
    if isinstance(text, str):  # Ελέγχει αν η τιμή είναι string
        tokens = word_tokenize(text.lower())  # tokenization και μετατροπή σε πεζούς χαρακτήρες
        filtered_tokens = [token for token in tokens if token.isalpha()]  # Φιλτράρει τα tokens που περιέχουν μόνο γράμματα
        tokens_list.append(filtered_tokens)
        
    else:
        
        pass

'''
# Εκτύπωση της λίστας των tokens
for i, filtered_tokens in enumerate(tokens_list):
    print("Tokens of inscription", i+1, ":", filtered_tokens)
'''

# Δημιουργία ενός TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Χρησιμοποιήστε μόνο τις κορυφαίες 1000 λέξεις

# Εφαρμογή του TfidfVectorizer στα tokens_list
tfidf_matrix = tfidf_vectorizer.fit_transform([" ".join(filtered_tokens) for filtered_tokens in tokens_list])

# Εκτύπωση του σχήματος του tfidf_matrix
print("Shape of TF-IDF matrix:", tfidf_matrix.shape)


# β ερωτημα 

# Κανονικοποίηση του TF-IDF matrix
scaler = MaxAbsScaler()
tfidf_matrix_normalized = scaler.fit_transform(tfidf_matrix)

# Κανονικοποίηση των χαρακτηριστικών date_min και date_max
data[['date_min', 'date_max']] = scaler.fit_transform(data[['date_min', 'date_max']])


# Εκτύπωση του σχήματος του κανονικοποιημένου tfidf_matrix
print("Shape of normalized TF-IDF matrix:", tfidf_matrix_normalized.shape)




#γ ερωτημα 



# Διαχωρισμός των δεδομένων σε σύνολα εκπαίδευσης και ελέγχου με χρήση K-Fold cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)

 # Αποθηκεύστε τα RMSE για κάθε fold
rmse_scores = []
#α4 
# Ορίσε τις πιθανότητες διατήρησης rin και rh
rin_values = [0.8, 0.5, 0.8]
rh_values = [0.5, 0.5, 0.2]

for kf, (train_index, test_index) in enumerate(kf.split(tfidf_matrix_normalized)):
    X_train, X_test = tfidf_matrix_normalized[train_index], tfidf_matrix_normalized[test_index]
    y_train, y_test = data[['date_min', 'date_max']].iloc[train_index], data[['date_min', 'date_max']].iloc[test_index]

#A3
# Ορισμός των υπερπαραμέτρων προς βελτιστοποίηση
    for rin, rh in zip(rin_values, rh_values):
        param_grid = {
            'learning_rate_init': [0.001, 0.001, 0.05 , 0.1],  # Διάφορες τιμές του ρυθμού εκπαίδευσης
            'momentum': [0.2, 0.6, 0.6 , 0.6] , # Διάφορες τιμές της σταθεράς ορμής
            'alpha': [1/(1-rin), 1/(1-rh)]
       }

   # Εκπαίδευση του ΤΝΔ
    model = MLPRegressor(hidden_layer_sizes=(10), max_iter=1000, activation='relu', learning_rate_init=0.001, random_state=42)
    model.fit(X_train, y_train)
    
        # Δημιουργία αντικειμένου GridSearchCV
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_root_mean_squared_error', cv=5)

     # Εκπαίδευση του GridSearchCV
    grid_search.fit(X_train, y_train)

   

  # ΕΡΩΤΗΜΑ Α2
    
    # Πρόβλεψη
    y_pred = model.predict(X_test)
    
    # Υπολογισμός RMSE
    rmse = np.sqrt(np.mean(np.square(custom_error(y_test, y_pred))))

  # Αποθηκεύστε το RMSE για το τρέχον fold
    rmse_scores.append(rmse)


    model.out_activation_ = 'identity'
    
'''
     # Παρουσίαση γραφικών παραστάσεων για τη σύγκλιση
    train_errors = np.zeros(model.max_iter)
    for i in range(model.max_iter):
            model.partial_fit(X_train, y_train)
            train_errors[i] = np.sqrt(mean_squared_error(y_train, model.predict(X_train)))
    plt.plot(range(1, len(train_errors) + 1), train_errors, label=f'{kf}')

    print(f"Fold {kf+1}: RMSE = {rmse}")
'''
    


# Εκτύπωση των βέλτιστων υπερπαραμέτρων
print("Best parameters:", grid_search.best_params_)

# Εκτύπωση της βέλτιστης απόδοσης (RMSE)
print("Best RMSE:", -grid_search.best_score_)

# Υπολογίστε τον μέσο όρο των RMSE για όλα τα folds
mean_rmse = np.mean(rmse_scores)
print(f"Overall RMSE with (10) hidden neurons = {mean_rmse}")# Σχεδίαση των γραφικών παραστάσεων σύγκλισης (μέσης RMSE ανά κύκλο εκπαίδευσης)
# Εκτύπωση του μέσου RMSE για κάθε ζευγάρι τιμών
print("Mean RMSE for each rin, rh pair:")
for i, (rin, rh) in enumerate(zip(rin_values, rh_values)):
    print(f"rin = {rin}, rh = {rh}: {rmse_scores[i]}")

    
'''
print("Grid search results:")
for params, score in zip(grid_search.cv_results_['params'], grid_search.cv_results_['mean_test_score']):
    print("Parameters:", params)
    print("Mean RMSE:", -score)
    print()




# Προσθήκη ετικετών και τίτλου στο γράφημα
plt.xlabel('Epochs')
plt.ylabel('RMSE')
plt.title('Convergence Rate (Mean RMSE per Epoch)')
plt.legend()
plt.show()
'''

